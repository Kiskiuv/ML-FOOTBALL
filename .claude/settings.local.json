{
  "permissions": {
    "allow": [
      "Bash(tail:*)",
      "Bash(cd \"C:/Users/marcg/Documents/ml-football-betting\" && python -u << 'PYEOF'\nimport pandas as pd\nimport os\nimport glob\nimport warnings\nwarnings.filterwarnings\\('ignore'\\)\n\nDATA_DIR = \"C:/Users/marcg/Documents/ml-football-betting/data\"\n\n# ============================================================\n# SECTION 0: INVENTORY OF ALL FILES\n# ============================================================\nprint\\(\"=\" * 90\\)\nprint\\(\"SECTION 0: FILE INVENTORY\"\\)\nprint\\(\"=\" * 90\\)\nall_files = sorted\\(os.listdir\\(DATA_DIR\\)\\)\nprint\\(f\"\\\\nTotal files: {len\\(all_files\\)}\"\\)\n\n# Classify unusual naming\ndouble_underscore = [f for f in all_files if '__' in f]\nspaces_in_name = [f for f in all_files if ' ' in f]\ncsv_files = [f for f in all_files if f.endswith\\('.csv'\\)]\nxls_files = [f for f in all_files if f.endswith\\('.xls'\\)]\nother_files = [f for f in all_files if not f.endswith\\('.csv'\\) and not f.endswith\\('.xls'\\)]\nno_underscore = [f for f in all_files if 'Matches' in f and not f.startswith\\('Matches_'\\) and not f.startswith\\('Matches__'\\)]\n\nprint\\(f\"\\\\n  .xls files: {len\\(xls_files\\)}\"\\)\nprint\\(f\"  .csv files: {len\\(csv_files\\)}\"\\)\nprint\\(f\"  Other:      {len\\(other_files\\)}\"\\)\n\nprint\\(f\"\\\\n  Files with DOUBLE UNDERSCORE: {double_underscore}\"\\)\nprint\\(f\"  Files with SPACES in name:    {spaces_in_name}\"\\)\nprint\\(f\"  CSV files \\(not .xls\\):         {csv_files}\"\\)\nprint\\(f\"  No standard underscore:       {no_underscore}\"\\)\n\n# ============================================================\n# SECTION 1: SAMPLE LEAGUE ANALYSIS\n# ============================================================\nprint\\(\"\\\\n\" + \"=\" * 90\\)\nprint\\(\"SECTION 1: SAMPLE LEAGUE ANALYSIS \\(SP1, D1, E0, F1, I1, ARGENTINA, MEXICO\\)\"\\)\nprint\\(\"=\" * 90\\)\n\nsample_files = {\n    'SP1':       os.path.join\\(DATA_DIR, 'Matches_SP1.xls'\\),\n    'D1':        os.path.join\\(DATA_DIR, 'Matches_D1.xls'\\),\n    'E0':        os.path.join\\(DATA_DIR, 'Matches__E0.xls'\\),  # double underscore!\n    'F1':        os.path.join\\(DATA_DIR, 'Matches_F1.xls'\\),\n    'I1':        os.path.join\\(DATA_DIR, 'Matches_I1.xls'\\),\n    'ARGENTINA': os.path.join\\(DATA_DIR, 'Matches_ARGENTINA.xls'\\),\n    'MEXICO':    os.path.join\\(DATA_DIR, 'Matches_MEXICO.xls'\\),\n}\n\ndfs = {}\nfor league, path in sample_files.items\\(\\):\n    try:\n        if path.endswith\\('.csv'\\):\n            df = pd.read_csv\\(path\\)\n        else:\n            df = pd.read_excel\\(path\\)\n        dfs[league] = df\n        print\\(f\"\\\\n  [{league}] Loaded: {os.path.basename\\(path\\)} — {df.shape[0]} rows x {df.shape[1]} cols\"\\)\n    except Exception as e:\n        print\\(f\"\\\\n  [{league}] FAILED to load {os.path.basename\\(path\\)}: {e}\"\\)\n\n# ---- 1a: Column names comparison ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1a. COLUMN NAMES PER LEAGUE\"\\)\nprint\\(\"-\" * 90\\)\n\nall_cols = {}\nfor league, df in dfs.items\\(\\):\n    cols = list\\(df.columns\\)\n    all_cols[league] = set\\(cols\\)\n    print\\(f\"\\\\n  [{league}] \\({len\\(cols\\)} columns\\):\"\\)\n    print\\(f\"    {cols}\"\\)\n\n# Find common and unique columns\nif len\\(all_cols\\) >= 2:\n    common = set.intersection\\(*all_cols.values\\(\\)\\)\n    print\\(f\"\\\\n  COMMON columns across ALL sampled leagues \\({len\\(common\\)}\\):\"\\)\n    print\\(f\"    {sorted\\(common\\)}\"\\)\n    \n    for league, cols in all_cols.items\\(\\):\n        unique = cols - common\n        if unique:\n            print\\(f\"  UNIQUE to [{league}]: {sorted\\(unique\\)}\"\\)\n\n# ---- 1b: Row counts and date ranges ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1b. ROW COUNTS AND DATE RANGES\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    date_col = None\n    for candidate in ['Date', 'MatchDate', 'date']:\n        if candidate in df.columns:\n            date_col = candidate\n            break\n    \n    if date_col:\n        dates = pd.to_datetime\\(df[date_col], dayfirst=True, errors='coerce'\\)\n        valid_dates = dates.dropna\\(\\)\n        min_d = valid_dates.min\\(\\) if len\\(valid_dates\\) > 0 else 'N/A'\n        max_d = valid_dates.max\\(\\) if len\\(valid_dates\\) > 0 else 'N/A'\n        null_dates = dates.isna\\(\\).sum\\(\\)\n        print\\(f\"  [{league}] {df.shape[0]:>6} rows | Date col: '{date_col}' | \"\n              f\"Range: {min_d} -> {max_d} | Unparseable dates: {null_dates}\"\\)\n    else:\n        print\\(f\"  [{league}] {df.shape[0]:>6} rows | NO date column found among {list\\(df.columns[:5]\\)}\"\\)\n\n# ---- 1c: Missing values ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1c. MISSING VALUES \\(% per column, showing cols with >0% missing\\)\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    missing = \\(df.isnull\\(\\).sum\\(\\) / len\\(df\\) * 100\\).round\\(2\\)\n    missing_cols = missing[missing > 0].sort_values\\(ascending=False\\)\n    if len\\(missing_cols\\) > 0:\n        print\\(f\"\\\\n  [{league}] — {len\\(missing_cols\\)} columns have missing values:\"\\)\n        for col, pct in missing_cols.items\\(\\):\n            print\\(f\"    {col:>25}: {pct:>6.1f}%  \\({df[col].isnull\\(\\).sum\\(\\)} of {len\\(df\\)}\\)\"\\)\n    else:\n        print\\(f\"\\\\n  [{league}] — NO missing values\"\\)\n\n# ---- 1d: Data types ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1d. DATA TYPES PER COLUMN \\(grouped by dtype\\)\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    dtype_groups = df.columns.to_series\\(\\).groupby\\(df.dtypes\\).apply\\(list\\)\n    print\\(f\"\\\\n  [{league}]:\"\\)\n    for dtype, cols in dtype_groups.items\\(\\):\n        print\\(f\"    {str\\(dtype\\):>10}: {cols[:15]}{'...' if len\\(cols\\) > 15 else ''} \\({len\\(cols\\)} cols\\)\"\\)\n\n# ---- 1e: Duplicate rows ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1e. DUPLICATE ROWS\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    n_dup = df.duplicated\\(\\).sum\\(\\)\n    if n_dup > 0:\n        print\\(f\"  [{league}] {n_dup} duplicate rows found!\"\\)\n        # Show a sample\n        dup_rows = df[df.duplicated\\(keep=False\\)]\n        print\\(f\"    Sample duplicate:\"\\)\n        for col in ['Date', 'HomeTeam', 'AwayTeam', 'Home', 'Away']:\n            if col in dup_rows.columns:\n                print\\(f\"      {dup_rows.iloc[0][[c for c in ['Date','HomeTeam','AwayTeam','Home','Away','FTHG','FTAG','FTR','HG','AG','Res'] if c in dup_rows.columns]]}\"\\)\n                break\n    else:\n        print\\(f\"  [{league}] No duplicate rows\"\\)\n\nPYEOF)",
      "Bash(cd:*)",
      "Bash(cd \"C:/Users/marcg/Documents/ml-football-betting\" && python -u << 'PYEOF'\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings\\('ignore'\\)\n\nDATA_DIR = \"C:/Users/marcg/Documents/ml-football-betting/data\"\n\ndef load_file\\(path\\):\n    \"\"\"Try multiple strategies to load a data file.\"\"\"\n    # First check if it's actually a CSV disguised as .xls\n    try:\n        with open\\(path, 'rb'\\) as f:\n            header = f.read\\(50\\)\n        if header[:4] != b'\\\\xd0\\\\xcf\\\\x11\\\\xe0':  # Not a real XLS \\(OLE2 magic\\)\n            # Try as CSV\n            for sep in [',', ';', '\\\\t']:\n                try:\n                    df = pd.read_csv\\(path, sep=sep, encoding='utf-8'\\)\n                    if df.shape[1] > 2:\n                        return df, f'CSV\\(sep={repr\\(sep\\)}\\)'\n                except:\n                    pass\n            for sep in [',', ';', '\\\\t']:\n                try:\n                    df = pd.read_csv\\(path, sep=sep, encoding='latin-1'\\)\n                    if df.shape[1] > 2:\n                        return df, f'CSV\\(sep={repr\\(sep\\)},latin1\\)'\n                except:\n                    pass\n        else:\n            df = pd.read_excel\\(path, engine='xlrd'\\)\n            return df, 'XLS\\(xlrd\\)'\n    except Exception as e:\n        pass\n    raise ValueError\\(f\"Could not load {path}\"\\)\n\n# ============================================================\n# SECTION 0: INVENTORY\n# ============================================================\nprint\\(\"=\" * 90\\)\nprint\\(\"SECTION 0: FILE INVENTORY\"\\)\nprint\\(\"=\" * 90\\)\nall_files = sorted\\(os.listdir\\(DATA_DIR\\)\\)\nprint\\(f\"\\\\nTotal files: {len\\(all_files\\)}\\\\n\"\\)\n\ndouble_underscore = [f for f in all_files if '__' in f]\nspaces_in_name = [f for f in all_files if ' ' in f]\ncsv_ext = [f for f in all_files if f.endswith\\('.csv'\\)]\nxls_ext = [f for f in all_files if f.endswith\\('.xls'\\)]\nno_std_underscore = [f for f in all_files if 'Matches' in f and not f.startswith\\('Matches_'\\) and not f.startswith\\('Matches__'\\)]\n\nprint\\(f\"  .xls extension: {len\\(xls_ext\\)}\"\\)\nprint\\(f\"  .csv extension: {len\\(csv_ext\\)}\"\\)\nprint\\(f\"\\\\n  NAMING ANOMALIES:\"\\)\nprint\\(f\"    Double underscore \\(__\\): {double_underscore}\"\\)\nprint\\(f\"    Spaces in name:        {spaces_in_name}\"\\)\nprint\\(f\"    No standard prefix:    {no_std_underscore}\"\\)\nprint\\(f\"    CSV extension:         {csv_ext}\"\\)\n\n# Check actual file format for each\nprint\\(f\"\\\\n  ACTUAL FILE FORMAT \\(magic bytes check\\):\"\\)\nfor f in all_files:\n    path = os.path.join\\(DATA_DIR, f\\)\n    with open\\(path, 'rb'\\) as fh:\n        magic = fh.read\\(8\\)\n    if magic[:4] == b'\\\\xd0\\\\xcf\\\\x11\\\\xe0':\n        fmt = \"Real XLS \\(OLE2\\)\"\n    elif magic[:4] == b'PK\\\\x03\\\\x04':\n        fmt = \"Real XLSX \\(ZIP/OOXML\\)\"\n    else:\n        # Likely CSV\n        try:\n            first_line = magic.decode\\('utf-8', errors='replace'\\)\n            fmt = f\"Text/CSV \\(starts with: '{first_line[:30]}...'\\)\"\n        except:\n            fmt = f\"Unknown \\(magic: {magic.hex\\(\\)[:16]}\\)\"\n    print\\(f\"    {f:>35}: {fmt}\"\\)\n\n# ============================================================\n# SECTION 1: SAMPLE LEAGUES\n# ============================================================\nprint\\(\"\\\\n\" + \"=\" * 90\\)\nprint\\(\"SECTION 1: DETAILED ANALYSIS OF SAMPLE LEAGUES\"\\)\nprint\\(\"=\" * 90\\)\n\nsample_files = {\n    'SP1':       'Matches_SP1.xls',\n    'D1':        'Matches_D1.xls',\n    'E0':        'Matches__E0.xls',\n    'F1':        'Matches_F1.xls',\n    'I1':        'Matches_I1.xls',\n    'ARGENTINA': 'Matches_ARGENTINA.xls',\n    'MEXICO':    'Matches_MEXICO.xls',\n}\n\ndfs = {}\nfor league, fname in sample_files.items\\(\\):\n    path = os.path.join\\(DATA_DIR, fname\\)\n    try:\n        df, method = load_file\\(path\\)\n        dfs[league] = df\n        print\\(f\"\\\\n  [{league}] Loaded via {method}: {fname} — {df.shape[0]} rows x {df.shape[1]} cols\"\\)\n    except Exception as e:\n        print\\(f\"\\\\n  [{league}] FAILED: {fname}: {e}\"\\)\n\n# ---- 1a: Column names ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1a. COLUMN NAMES PER LEAGUE\"\\)\nprint\\(\"-\" * 90\\)\n\nall_cols = {}\nfor league, df in dfs.items\\(\\):\n    cols = list\\(df.columns\\)\n    all_cols[league] = set\\(cols\\)\n    print\\(f\"\\\\n  [{league}] \\({len\\(cols\\)} columns\\):\"\\)\n    # Print in groups of 10 for readability\n    for i in range\\(0, len\\(cols\\), 10\\):\n        print\\(f\"    {cols[i:i+10]}\"\\)\n\nif len\\(all_cols\\) >= 2:\n    common = set.intersection\\(*all_cols.values\\(\\)\\)\n    union_all = set.union\\(*all_cols.values\\(\\)\\)\n    print\\(f\"\\\\n  COMMON columns across ALL {len\\(all_cols\\)} sampled leagues \\({len\\(common\\)}\\):\"\\)\n    print\\(f\"    {sorted\\(common\\)}\"\\)\n    \n    print\\(f\"\\\\n  COLUMNS UNIQUE TO SPECIFIC LEAGUES:\"\\)\n    for league, cols in all_cols.items\\(\\):\n        unique = cols - common\n        if unique:\n            print\\(f\"    [{league}] unique \\({len\\(unique\\)}\\): {sorted\\(unique\\)}\"\\)\n\n# ---- 1b: Row counts and date ranges ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1b. ROW COUNTS AND DATE RANGES\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    date_col = None\n    for candidate in ['Date', 'MatchDate', 'date', 'DATE']:\n        if candidate in df.columns:\n            date_col = candidate\n            break\n    \n    if date_col:\n        raw_sample = df[date_col].dropna\\(\\).head\\(3\\).tolist\\(\\)\n        dates = pd.to_datetime\\(df[date_col], dayfirst=True, errors='coerce'\\)\n        valid_dates = dates.dropna\\(\\)\n        null_dates = dates.isna\\(\\).sum\\(\\)\n        if len\\(valid_dates\\) > 0:\n            min_d = valid_dates.min\\(\\).strftime\\('%Y-%m-%d'\\)\n            max_d = valid_dates.max\\(\\).strftime\\('%Y-%m-%d'\\)\n        else:\n            min_d = max_d = 'N/A'\n        print\\(f\"  [{league:>10}] {df.shape[0]:>6} rows | Col: '{date_col}' | \"\n              f\"{min_d} -> {max_d} | Bad dates: {null_dates} | \"\n              f\"Samples: {raw_sample}\"\\)\n    else:\n        print\\(f\"  [{league:>10}] {df.shape[0]:>6} rows | NO date column! Cols: {list\\(df.columns[:8]\\)}\"\\)\n\n# ---- 1c: Missing values ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1c. MISSING VALUES \\(% per column, cols with >0% missing\\)\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    missing = \\(df.isnull\\(\\).sum\\(\\) / len\\(df\\) * 100\\).round\\(2\\)\n    missing_cols = missing[missing > 0].sort_values\\(ascending=False\\)\n    if len\\(missing_cols\\) > 0:\n        print\\(f\"\\\\n  [{league}] — {len\\(missing_cols\\)} cols with missing values \\(showing top 20\\):\"\\)\n        for i, \\(col, pct\\) in enumerate\\(missing_cols.items\\(\\)\\):\n            if i >= 20:\n                print\\(f\"    ... and {len\\(missing_cols\\) - 20} more columns with missing data\"\\)\n                break\n            print\\(f\"    {col:>25}: {pct:>6.1f}%  \\({df[col].isnull\\(\\).sum\\(\\):>5}/{len\\(df\\)}\\)\"\\)\n    else:\n        print\\(f\"\\\\n  [{league}] — NO missing values \\(clean!\\)\"\\)\n\n# ---- 1d: Data types ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1d. DATA TYPES SUMMARY\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    dtype_counts = df.dtypes.value_counts\\(\\)\n    print\\(f\"\\\\n  [{league}]:\"\\)\n    for dtype, count in dtype_counts.items\\(\\):\n        cols = df.select_dtypes\\(include=[dtype]\\).columns.tolist\\(\\)\n        displayed = cols[:8]\n        more = f\" + {len\\(cols\\)-8} more\" if len\\(cols\\) > 8 else \"\"\n        print\\(f\"    {str\\(dtype\\):>10} \\({count:>3} cols\\): {displayed}{more}\"\\)\n\n# ---- 1e: Duplicates ----\nprint\\(\"\\\\n\" + \"-\" * 90\\)\nprint\\(\"1e. DUPLICATE ROWS\"\\)\nprint\\(\"-\" * 90\\)\n\nfor league, df in dfs.items\\(\\):\n    n_dup = df.duplicated\\(\\).sum\\(\\)\n    # Also check for duplicates based on key columns\n    key_cols_options = [\n        ['MatchDate', 'HomeTeam', 'AwayTeam'],\n        ['Date', 'HomeTeam', 'AwayTeam'],\n        ['Date', 'Home', 'Away'],\n    ]\n    key_dup = None\n    key_used = None\n    for key_cols in key_cols_options:\n        if all\\(c in df.columns for c in key_cols\\):\n            key_dup = df.duplicated\\(subset=key_cols\\).sum\\(\\)\n            key_used = key_cols\n            break\n    \n    print\\(f\"  [{league:>10}] Full row dups: {n_dup:>4}\", end=\"\"\\)\n    if key_dup is not None:\n        print\\(f\" | Key dups \\({key_used}\\): {key_dup}\", end=\"\"\\)\n    print\\(\\)\n    \n    if n_dup > 0:\n        dup_mask = df.duplicated\\(keep=False\\)\n        dup_df = df[dup_mask]\n        show_cols = [c for c in ['MatchDate', 'Date', 'HomeTeam', 'AwayTeam', 'Home', 'Away', 'FTHG', 'FTAG', 'FTR', 'HG', 'AG', 'Res'] if c in df.columns]\n        if show_cols:\n            print\\(f\"    Sample duplicates:\"\\)\n            print\\(f\"    {dup_df[show_cols].head\\(4\\).to_string\\(\\)}\"\\)\n\nPYEOF)",
      "Bash(cd \"C:/Users/marcg/Documents/ml-football-betting\" && python _analyze_features.py 2>&1)",
      "Bash(python:*)",
      "Bash(echo === DATA FILES ===:*)",
      "Bash(ls:*)",
      "Bash(python3:*)",
      "Bash(cat:*)"
    ]
  }
}
